{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9035e6-420d-47bc-8325-a4d5a957dc5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Monitoring Performance of Jobs\n",
    "\n",
    "Optimizing performance from a job and cluster perspective is a critical part of Production Delta Lake Performance. We often use a variety of methods listed in the table below to monitor jobs performance.\n",
    "\n",
    "|Feature|Use|Link|\n",
    "|-------|---|----|\n",
    "|[Ganglia ]Metrics: Cluster Tuning | Controlling the knobs associated with seeking out maximal performance|https://docs.databricks.com/clusters/configure.html, https://www.youtube.com/watch?v=9fa8dnKbfsU|\n",
    "|Spark UI: DAG Tuning | Controlling the stages associated with a Spark Job and a Query Execution Plan | https://databricks.com/session/understanding-query-plans-and-spark-uis|\n",
    "|Logs | Spark Driver and Cluster Logs that provide execution and autoscaling details | https://docs.databricks.com/spark/latest/rdd-streaming/debugging-streaming-applications.html|\n",
    "|EXPLAIN: Physical Plan | physical plan provides the fundamental information about the execution of the query |https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-explain.html |\n",
    "\n",
    "Useful blogs to understand how Delta Lake performance tuning works are listed below.\n",
    "* [Photon, The Next Generation Query Engine on the Databricks Lakehouse Platform](https://www.databricks.com/blog/2021/06/17/announcing-photon-public-preview-the-next-generation-query-engine-on-the-databricks-lakehouse-platform.html)\n",
    "* [Faster MERGE Performance With Low-Shuffle MERGE and Photon](https://www.databricks.com/blog/2022/10/17/faster-merge-performance-low-shuffle-merge-and-photon.html)\n",
    "* [Understanding your Apache Spark Application Through Visualization](https://www.databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dbef2b8-83a0-48f6-a351-ba3535a58683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Cluster Performance\n",
    "\n",
    "Cluster performance tuning is an important step when quantifying delta performance. The distributed nature of Delta and Spark allow great horizontal scaling by adding more nodes to meet performance SLAs. Generally speaking, leverage autoscaling on Spark clusters to reduce costs and tackle peak throughput workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a02dbe-0034-44f0-bd2f-c0a0e6733bb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Live Metrics - Metrics UI\n",
    "\n",
    "Databricks cluster performance can be observed in the Ganglia UI (or Metrics UI) which runs live on the cluster. Ganglia metrics are available for Databricks Runtime 12.2 and below and the metrics replacement is now enabled on clusters with DBR 13.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/Ganglia%20UI.png' width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db24b0a4-1f23-4cd7-8e1b-a74df9e84a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Spark UI\n",
    "\n",
    "Databricks exposes the Spark UI which will provide a large amount of usable statistics to measure the performance of your jobs. Every job in Spark consists of a series of spark tasks (stages) which form a directed acyclic graph (DAG). Examining these DAGs can help identify bottleneck stages to determine where more performance can be extracted. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/Spark%20UI-%20Job%20159.png' width=\"800\">\n",
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/Spark%20UI-%20Job%20166%20-%20No%20Photon.png' width=\"1300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65b8ac2-9ad9-4c55-bc42-e34cc0866502",
     "showTitle": false,
     "title": "Speed up queries by identifying execution bottlenecks in Query Plans"
    }
   },
   "source": [
    "\n",
    "## Speed up queries by identifying execution bottlenecks in Query Plans\n",
    "A common methodology for speeding up queries is to first identify the longest running query operators. We are more interested in total time spent on a task rather than the exact “wall clock time” of an operator as we’re dealing with a distributed system and operators can be executed in parallel. Each query operator comes with a slew of statistics. In the case of a scan operator, metrics include number of files or data read, time spent waiting for cloud storage or time spent reading files. As a result, it is easy to answer questions such as which table should be optimized or whether a join could be improved. All blue DAGs in the query plan confirms that photon was disabled when the query ran. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/Databricks%20Shell%20-%20Details%20for%20Query%20299.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f868dc-70e8-4af4-b95b-78f916034721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nusername: mojgan.mazouchi@databricks.com\nuserhome: dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance\ndatabase: PhotonPerformance_mojgan_mazouchi_databricks_com_db\n"
     ]
    }
   ],
   "source": [
    "%run ./00-setup $mode=\"reuse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d86305b-eb29-46a5-ac05-ec51740766c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.photon.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.photon.parquetWriter.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.photon.window.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.photon.sort.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.photon.window.experimental.features.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd1d5ce-437e-44c2-9c81-32afd157e608",
     "showTitle": true,
     "title": "Run Explain when Photon is Disabled"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[== Physical Plan ==\n",
       "AdaptiveSparkPlan isFinalPlan=false\n",
       "+- HashAggregate(keys=[EmpLength#12737, IntRate#12741], functions=[finalmerge_avg(merge sum#12755, count#12756L) AS avg(loan_amnt#12601)#12747, finalmerge_min(merge min#12758) AS min(annual_inc#12612)#12748, finalmerge_max(merge max#12760) AS max(annual_inc#12612)#12749, finalmerge_sum(merge sum#12762) AS sum(cast(total_pymnt#12636 as double))#12750, finalmerge_count(distinct merge count#12752L) AS count(addr_state#12621)#12746L])\n",
       "   +- Exchange hashpartitioning(EmpLength#12737, IntRate#12741, 200), ENSURE_REQUIREMENTS, [plan_id=4388]\n",
       "      +- HashAggregate(keys=[EmpLength#12737, IntRate#12741], functions=[merge_avg(merge sum#12755, count#12756L) AS (sum#12755, count#12756L), merge_min(merge min#12758) AS min#12758, merge_max(merge max#12760) AS max#12760, merge_sum(merge sum#12762) AS sum#12762, partial_count(distinct addr_state#12621) AS count#12752L])\n",
       "         +- HashAggregate(keys=[EmpLength#12737, IntRate#12741, addr_state#12621], functions=[merge_avg(merge sum#12755, count#12756L) AS (sum#12755, count#12756L), merge_min(merge min#12758) AS min#12758, merge_max(merge max#12760) AS max#12760, merge_sum(merge sum#12762) AS sum#12762])\n",
       "            +- Exchange hashpartitioning(EmpLength#12737, IntRate#12741, addr_state#12621, 200), ENSURE_REQUIREMENTS, [plan_id=4384]\n",
       "               +- HashAggregate(keys=[EmpLength#12737, IntRate#12741, addr_state#12621], functions=[partial_avg(loan_amnt#12601) AS (sum#12755, count#12756L), partial_min(annual_inc#12612) AS min#12758, partial_max(annual_inc#12612) AS max#12760, partial_sum(cast(total_pymnt#12636 as double)) AS sum#12762])\n",
       "                  +- Project [loan_amnt#12601, annual_inc#12612, addr_state#12621, total_pymnt#12636, EmpLength#12737, IntRate#12741]\n",
       "                     +- BroadcastHashJoin [int_rate#12605], [int_rate#12740], LeftOuter, BuildRight, false\n",
       "                        :- Project [loan_amnt#12601, int_rate#12605, annual_inc#12612, addr_state#12621, total_pymnt#12636, EmpLength#12737]\n",
       "                        :  +- BroadcastHashJoin [emp_length#12610], [emp_length#12736], Inner, BuildRight, false\n",
       "                        :     :- Project [loan_amnt#12601, int_rate#12605, emp_length#12610, annual_inc#12612, addr_state#12621, total_pymnt#12636]\n",
       "                        :     :  +- Filter ((((isnotnull(annual_inc#12612) AND (annual_inc#12612 &gt; 16000.0)) AND isnotnull(loan_status#12614)) AND isnotnull(emp_length#12610)) AND (loan_status#12614 = Current))\n",
       "                        :     :     +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_silver[loan_amnt#12601,int_rate#12605,emp_length#12610,annual_inc#12612,loan_status#12614,addr_state#12621,total_pymnt#12636] Batched: true, DataFilters: [isnotnull(annual_inc#12612), (annual_inc#12612 &gt; 16000.0), isnotnull(loan_status#12614), isnotnu..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(annual_inc), GreaterThan(annual_inc,16000.0), IsNotNull(loan_status), IsNotNull(emp_le..., ReadSchema: struct&lt;loan_amnt:float,int_rate:string,emp_length:string,annual_inc:float,loan_status:string,addr...\n",
       "                        :     +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=4375]\n",
       "                        :        +- Project [emp_length#12736, EmpLength#12737]\n",
       "                        :           +- Filter ((((isnotnull(avg_cur_bal#12739) AND (avg_cur_bal#12739 &gt;= 1)) AND (avg_cur_bal#12739 &lt;= 2000)) AND isnotnull(emp_length#12736)) AND EmpLength#12737 IN (3-5Years,1year,Under1year))\n",
       "                        :              +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_emplength[emp_length#12736,EmpLength#12737,avg_cur_bal#12739] Batched: true, DataFilters: [isnotnull(avg_cur_bal#12739), (avg_cur_bal#12739 &gt;= 1), (avg_cur_bal#12739 &lt;= 2000), isnotnull(e..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(avg_cur_bal), GreaterThanOrEqual(avg_cur_bal,1), LessThanOrEqual(avg_cur_bal,2000), Is..., ReadSchema: struct&lt;emp_length:string,EmpLength:string,avg_cur_bal:int&gt;\n",
       "                        +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=4379]\n",
       "                           +- Filter isnotnull(int_rate#12740)\n",
       "                              +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_intrate[int_rate#12740,IntRate#12741] Batched: true, DataFilters: [isnotnull(int_rate#12740)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(int_rate)], ReadSchema: struct&lt;int_rate:string,IntRate:string&gt;\n",
       "\n",
       "]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[EmpLength#12737, IntRate#12741], functions=[finalmerge_avg(merge sum#12755, count#12756L) AS avg(loan_amnt#12601)#12747, finalmerge_min(merge min#12758) AS min(annual_inc#12612)#12748, finalmerge_max(merge max#12760) AS max(annual_inc#12612)#12749, finalmerge_sum(merge sum#12762) AS sum(cast(total_pymnt#12636 as double))#12750, finalmerge_count(distinct merge count#12752L) AS count(addr_state#12621)#12746L])\n   +- Exchange hashpartitioning(EmpLength#12737, IntRate#12741, 200), ENSURE_REQUIREMENTS, [plan_id=4388]\n      +- HashAggregate(keys=[EmpLength#12737, IntRate#12741], functions=[merge_avg(merge sum#12755, count#12756L) AS (sum#12755, count#12756L), merge_min(merge min#12758) AS min#12758, merge_max(merge max#12760) AS max#12760, merge_sum(merge sum#12762) AS sum#12762, partial_count(distinct addr_state#12621) AS count#12752L])\n         +- HashAggregate(keys=[EmpLength#12737, IntRate#12741, addr_state#12621], functions=[merge_avg(merge sum#12755, count#12756L) AS (sum#12755, count#12756L), merge_min(merge min#12758) AS min#12758, merge_max(merge max#12760) AS max#12760, merge_sum(merge sum#12762) AS sum#12762])\n            +- Exchange hashpartitioning(EmpLength#12737, IntRate#12741, addr_state#12621, 200), ENSURE_REQUIREMENTS, [plan_id=4384]\n               +- HashAggregate(keys=[EmpLength#12737, IntRate#12741, addr_state#12621], functions=[partial_avg(loan_amnt#12601) AS (sum#12755, count#12756L), partial_min(annual_inc#12612) AS min#12758, partial_max(annual_inc#12612) AS max#12760, partial_sum(cast(total_pymnt#12636 as double)) AS sum#12762])\n                  +- Project [loan_amnt#12601, annual_inc#12612, addr_state#12621, total_pymnt#12636, EmpLength#12737, IntRate#12741]\n                     +- BroadcastHashJoin [int_rate#12605], [int_rate#12740], LeftOuter, BuildRight, false\n                        :- Project [loan_amnt#12601, int_rate#12605, annual_inc#12612, addr_state#12621, total_pymnt#12636, EmpLength#12737]\n                        :  +- BroadcastHashJoin [emp_length#12610], [emp_length#12736], Inner, BuildRight, false\n                        :     :- Project [loan_amnt#12601, int_rate#12605, emp_length#12610, annual_inc#12612, addr_state#12621, total_pymnt#12636]\n                        :     :  +- Filter ((((isnotnull(annual_inc#12612) AND (annual_inc#12612 &gt; 16000.0)) AND isnotnull(loan_status#12614)) AND isnotnull(emp_length#12610)) AND (loan_status#12614 = Current))\n                        :     :     +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_silver[loan_amnt#12601,int_rate#12605,emp_length#12610,annual_inc#12612,loan_status#12614,addr_state#12621,total_pymnt#12636] Batched: true, DataFilters: [isnotnull(annual_inc#12612), (annual_inc#12612 &gt; 16000.0), isnotnull(loan_status#12614), isnotnu..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(annual_inc), GreaterThan(annual_inc,16000.0), IsNotNull(loan_status), IsNotNull(emp_le..., ReadSchema: struct&lt;loan_amnt:float,int_rate:string,emp_length:string,annual_inc:float,loan_status:string,addr...\n                        :     +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=4375]\n                        :        +- Project [emp_length#12736, EmpLength#12737]\n                        :           +- Filter ((((isnotnull(avg_cur_bal#12739) AND (avg_cur_bal#12739 &gt;= 1)) AND (avg_cur_bal#12739 &lt;= 2000)) AND isnotnull(emp_length#12736)) AND EmpLength#12737 IN (3-5Years,1year,Under1year))\n                        :              +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_emplength[emp_length#12736,EmpLength#12737,avg_cur_bal#12739] Batched: true, DataFilters: [isnotnull(avg_cur_bal#12739), (avg_cur_bal#12739 &gt;= 1), (avg_cur_bal#12739 &lt;= 2000), isnotnull(e..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(avg_cur_bal), GreaterThanOrEqual(avg_cur_bal,1), LessThanOrEqual(avg_cur_bal,2000), Is..., ReadSchema: struct&lt;emp_length:string,EmpLength:string,avg_cur_bal:int&gt;\n                        +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=4379]\n                           +- Filter isnotnull(int_rate#12740)\n                              +- FileScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_intrate[int_rate#12740,IntRate#12741] Batched: true, DataFilters: [isnotnull(int_rate#12740)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], PushedFilters: [IsNotNull(int_rate)], ReadSchema: struct&lt;int_rate:string,IntRate:string&gt;\n\n]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "spark.sql(\"\"\"EXPLAIN SELECT\n",
    "  T_len.EmpLength,\n",
    "  T_rate.IntRate,\n",
    "  count(DISTINCT T.addr_state) cnt_loan_by_state,\n",
    "  avg(loan_amnt) avg_loan_by_state,\n",
    "  min(DISTINCT annual_inc) as min_annual_income,\n",
    "  max(DISTINCT annual_inc) as max_annual_income,\n",
    "  sum(total_pymnt) totalPayment_by_state\n",
    "FROM\n",
    "  LendingClub_silver T\n",
    "  LEFT JOIN \n",
    "  (SELECT row_number() OVER(PARTITION BY addr_state ORDER BY avg_cur_bal DESC) as row_num_avgBal_state, *\n",
    "  FROM LendingClub_EmpLength) T_len on T_len.emp_length = T.emp_length and T_len.avg_cur_bal BETWEEN 1 AND 2000\n",
    "  LEFT JOIN LendingClub_IntRate T_rate on T_rate.int_rate = T.int_rate\n",
    "WHERE\n",
    "  (annual_inc> 16000) AND loan_status == \"Current\"\n",
    "GROUP BY\n",
    "  1,\n",
    "  2\n",
    "HAVING EmpLength IN ('3-5Years', '1year', 'Under1year')\"\"\").collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d2e37f-6153-4403-b0ee-9e2e9f777c54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Photon execution analysis in Query Plan\n",
    "If you are using Photon on Databricks clusters, you can view Photon action in the Spark UI. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with Photon, such as PhotonGroupingAgg. Secondly, in the DAG Photon operators and stages are colored orange, whereas the non-Photon ones are blue.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/Databricks%20Shell%20-%20Details%20for%20Query%20272.png' width=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98b3833-930f-47ad-b0f0-2ffd8024e497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Enable photon and it's support for sort and window functions\n",
    "spark.conf.set(\"spark.databricks.photon.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.photon.parquetWriter.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.photon.window.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.photon.sort.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.photon.window.experimental.features.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2386adcd-0e52-4b64-83fc-1b036762c882",
     "showTitle": true,
     "title": "Run Explain on Photon-Enabled Cluster"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[== Physical Plan ==\n",
       "AdaptiveSparkPlan isFinalPlan=false\n",
       "+- ColumnarToRow\n",
       "   +- PhotonResultStage\n",
       "      +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151], functions=[finalmerge_avg(merge sum#13165, count#13166L) AS avg(loan_amnt#13009)#13157, finalmerge_min(merge min#13168) AS min(annual_inc#13020)#13158, finalmerge_max(merge max#13170) AS max(annual_inc#13020)#13159, finalmerge_sum(merge sum#13172) AS sum(cast(total_pymnt#13044 as double))#13160, finalmerge_count(distinct merge count#13162L) AS count(addr_state#13029)#13156L])\n",
       "         +- PhotonShuffleExchangeSource\n",
       "            +- PhotonShuffleMapStage\n",
       "               +- PhotonShuffleExchangeSink hashpartitioning(EmpLength#13145, IntRate#13151, 200)\n",
       "                  +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151], functions=[merge_avg(merge sum#13165, count#13166L) AS (sum#13165, count#13166L), merge_min(merge min#13168) AS min#13168, merge_max(merge max#13170) AS max#13170, merge_sum(merge sum#13172) AS sum#13172, partial_count(distinct addr_state#13029) AS count#13162L])\n",
       "                     +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151, addr_state#13029], functions=[merge_avg(merge sum#13165, count#13166L) AS (sum#13165, count#13166L), merge_min(merge min#13168) AS min#13168, merge_max(merge max#13170) AS max#13170, merge_sum(merge sum#13172) AS sum#13172])\n",
       "                        +- PhotonShuffleExchangeSource\n",
       "                           +- PhotonShuffleMapStage\n",
       "                              +- PhotonShuffleExchangeSink hashpartitioning(EmpLength#13145, IntRate#13151, addr_state#13029, 200)\n",
       "                                 +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151, addr_state#13029], functions=[partial_avg(loan_amnt#13009) AS (sum#13165, count#13166L), partial_min(annual_inc#13020) AS min#13168, partial_max(annual_inc#13020) AS max#13170, partial_sum(cast(total_pymnt#13044 as double)) AS sum#13172])\n",
       "                                    +- PhotonProject [loan_amnt#13009, annual_inc#13020, addr_state#13029, total_pymnt#13044, EmpLength#13145, IntRate#13151]\n",
       "                                       +- PhotonBroadcastHashJoin [int_rate#13013], [int_rate#13150], LeftOuter, BuildRight, false, true\n",
       "                                          :- PhotonProject [loan_amnt#13009, int_rate#13013, annual_inc#13020, addr_state#13029, total_pymnt#13044, EmpLength#13145]\n",
       "                                          :  +- PhotonBroadcastHashJoin [emp_length#13018], [emp_length#13144], Inner, BuildRight, false, true\n",
       "                                          :     :- PhotonProject [loan_amnt#13009, int_rate#13013, emp_length#13018, annual_inc#13020, addr_state#13029, total_pymnt#13044]\n",
       "                                          :     :  +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_silver[loan_amnt#13009,int_rate#13013,emp_length#13018,annual_inc#13020,loan_status#13022,addr_state#13029,total_pymnt#13044] DataFilters: [isnotnull(annual_inc#13020), (annual_inc#13020 &gt; 16000.0), isnotnull(loan_status#13022), isnotnu..., DictionaryFilters: [(annual_inc#13020 &gt; 16000.0), (loan_status#13022 = Current)], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;loan_amnt:float,int_rate:string,emp_length:string,annual_inc:float,loan_status:string,addr..., RequiredDataFilters: [isnotnull(annual_inc#13020), (annual_inc#13020 &gt; 16000.0), isnotnull(loan_status#13022), isnotnu...\n",
       "                                          :     +- PhotonShuffleExchangeSource\n",
       "                                          :        +- PhotonShuffleMapStage\n",
       "                                          :           +- PhotonShuffleExchangeSink SinglePartition\n",
       "                                          :              +- PhotonProject [emp_length#13144, EmpLength#13145]\n",
       "                                          :                 +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_emplength[emp_length#13144,EmpLength#13145,avg_cur_bal#13147] DataFilters: [isnotnull(avg_cur_bal#13147), (avg_cur_bal#13147 &gt;= 1), (avg_cur_bal#13147 &lt;= 100), isnotnull(em..., DictionaryFilters: [EmpLength#13145 IN (3-5Years,1year,Under1year), ((avg_cur_bal#13147 &gt;= 1) AND (avg_cur_bal#13147..., Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;emp_length:string,EmpLength:string,avg_cur_bal:int&gt;, RequiredDataFilters: [isnotnull(avg_cur_bal#13147), (avg_cur_bal#13147 &gt;= 1), (avg_cur_bal#13147 &lt;= 100), isnotnull(em...\n",
       "                                          +- PhotonShuffleExchangeSource\n",
       "                                             +- PhotonShuffleMapStage\n",
       "                                                +- PhotonShuffleExchangeSink SinglePartition\n",
       "                                                   +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_intrate[int_rate#13150,IntRate#13151] DataFilters: [isnotnull(int_rate#13150)], DictionaryFilters: [], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;int_rate:string,IntRate:string&gt;, RequiredDataFilters: [isnotnull(int_rate#13150)]\n",
       "\n",
       "\n",
       "== Photon Explanation ==\n",
       "The query is fully supported by Photon.]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151], functions=[finalmerge_avg(merge sum#13165, count#13166L) AS avg(loan_amnt#13009)#13157, finalmerge_min(merge min#13168) AS min(annual_inc#13020)#13158, finalmerge_max(merge max#13170) AS max(annual_inc#13020)#13159, finalmerge_sum(merge sum#13172) AS sum(cast(total_pymnt#13044 as double))#13160, finalmerge_count(distinct merge count#13162L) AS count(addr_state#13029)#13156L])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage\n               +- PhotonShuffleExchangeSink hashpartitioning(EmpLength#13145, IntRate#13151, 200)\n                  +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151], functions=[merge_avg(merge sum#13165, count#13166L) AS (sum#13165, count#13166L), merge_min(merge min#13168) AS min#13168, merge_max(merge max#13170) AS max#13170, merge_sum(merge sum#13172) AS sum#13172, partial_count(distinct addr_state#13029) AS count#13162L])\n                     +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151, addr_state#13029], functions=[merge_avg(merge sum#13165, count#13166L) AS (sum#13165, count#13166L), merge_min(merge min#13168) AS min#13168, merge_max(merge max#13170) AS max#13170, merge_sum(merge sum#13172) AS sum#13172])\n                        +- PhotonShuffleExchangeSource\n                           +- PhotonShuffleMapStage\n                              +- PhotonShuffleExchangeSink hashpartitioning(EmpLength#13145, IntRate#13151, addr_state#13029, 200)\n                                 +- PhotonGroupingAgg(keys=[EmpLength#13145, IntRate#13151, addr_state#13029], functions=[partial_avg(loan_amnt#13009) AS (sum#13165, count#13166L), partial_min(annual_inc#13020) AS min#13168, partial_max(annual_inc#13020) AS max#13170, partial_sum(cast(total_pymnt#13044 as double)) AS sum#13172])\n                                    +- PhotonProject [loan_amnt#13009, annual_inc#13020, addr_state#13029, total_pymnt#13044, EmpLength#13145, IntRate#13151]\n                                       +- PhotonBroadcastHashJoin [int_rate#13013], [int_rate#13150], LeftOuter, BuildRight, false, true\n                                          :- PhotonProject [loan_amnt#13009, int_rate#13013, annual_inc#13020, addr_state#13029, total_pymnt#13044, EmpLength#13145]\n                                          :  +- PhotonBroadcastHashJoin [emp_length#13018], [emp_length#13144], Inner, BuildRight, false, true\n                                          :     :- PhotonProject [loan_amnt#13009, int_rate#13013, emp_length#13018, annual_inc#13020, addr_state#13029, total_pymnt#13044]\n                                          :     :  +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_silver[loan_amnt#13009,int_rate#13013,emp_length#13018,annual_inc#13020,loan_status#13022,addr_state#13029,total_pymnt#13044] DataFilters: [isnotnull(annual_inc#13020), (annual_inc#13020 &gt; 16000.0), isnotnull(loan_status#13022), isnotnu..., DictionaryFilters: [(annual_inc#13020 &gt; 16000.0), (loan_status#13022 = Current)], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;loan_amnt:float,int_rate:string,emp_length:string,annual_inc:float,loan_status:string,addr..., RequiredDataFilters: [isnotnull(annual_inc#13020), (annual_inc#13020 &gt; 16000.0), isnotnull(loan_status#13022), isnotnu...\n                                          :     +- PhotonShuffleExchangeSource\n                                          :        +- PhotonShuffleMapStage\n                                          :           +- PhotonShuffleExchangeSink SinglePartition\n                                          :              +- PhotonProject [emp_length#13144, EmpLength#13145]\n                                          :                 +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_emplength[emp_length#13144,EmpLength#13145,avg_cur_bal#13147] DataFilters: [isnotnull(avg_cur_bal#13147), (avg_cur_bal#13147 &gt;= 1), (avg_cur_bal#13147 &lt;= 100), isnotnull(em..., DictionaryFilters: [EmpLength#13145 IN (3-5Years,1year,Under1year), ((avg_cur_bal#13147 &gt;= 1) AND (avg_cur_bal#13147..., Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;emp_length:string,EmpLength:string,avg_cur_bal:int&gt;, RequiredDataFilters: [isnotnull(avg_cur_bal#13147), (avg_cur_bal#13147 &gt;= 1), (avg_cur_bal#13147 &lt;= 100), isnotnull(em...\n                                          +- PhotonShuffleExchangeSource\n                                             +- PhotonShuffleMapStage\n                                                +- PhotonShuffleExchangeSink SinglePartition\n                                                   +- PhotonScan parquet hive_metastore.photonperformance_mojgan_mazouchi_databricks_com_db.lendingclub_intrate[int_rate#13150,IntRate#13151] DataFilters: [isnotnull(int_rate#13150)], DictionaryFilters: [], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/mojgan.mazouchi@databricks.com/PhotonPerformance/lendi..., PartitionFilters: [], ReadSchema: struct&lt;int_rate:string,IntRate:string&gt;, RequiredDataFilters: [isnotnull(int_rate#13150)]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "spark.sql(\"\"\"EXPLAIN SELECT\n",
    "  T_len.EmpLength,\n",
    "  T_rate.IntRate,\n",
    "  count(DISTINCT T.addr_state) cnt_loan_by_state,\n",
    "  avg(loan_amnt) avg_loan_by_state,\n",
    "  min(DISTINCT annual_inc) as min_annual_income,\n",
    "  max(DISTINCT annual_inc) as max_annual_income,\n",
    "  sum(total_pymnt) totalPayment_by_state\n",
    "FROM\n",
    "  LendingClub_silver T\n",
    "  LEFT JOIN \n",
    "  (SELECT row_number() OVER(PARTITION BY addr_state ORDER BY avg_cur_bal DESC) as row_num_avgBal_state, *\n",
    "  FROM LendingClub_EmpLength) T_len on T_len.emp_length = T.emp_length and T_len.avg_cur_bal BETWEEN 1 AND 100\n",
    "  LEFT JOIN LendingClub_IntRate T_rate on T_rate.int_rate = T.int_rate\n",
    "WHERE\n",
    "  (annual_inc> 16000) AND loan_status == \"Current\"\n",
    "GROUP BY\n",
    "  1,\n",
    "  2\n",
    "HAVING EmpLength IN ('3-5Years', '1year', 'Under1year')\"\"\").collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "674cb4ee-88fa-4315-a480-e5f98dc4e7a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Photon-Enabled Clusters\n",
    "\n",
    "By enabling the advice text (`set spark.databricks.adviceGenerator.acceleratedWithPhoton.enabled = true;`), you can trace photon-enabled clusters logs in the INFO section of Driver logs under Log4j output. Look specifically for \"Accelerated with photon\" in the logs to find out how much your queries and workloads accelerated by photon.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> \n",
    "** Advice text is disabled by default**, and you have to enable it in advance, prior to running your queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f1ba4a7-830c-4270-97d9-0cc35d718f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/morganmazouchi/Performance-with-Photon/main/Images/log%204j%20output.png' width=\"2500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f43ab4-7914-4c00-a2ad-fc139f8fbb84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.adviceGenerator.acceleratedWithPhoton.enabled</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.adviceGenerator.acceleratedWithPhoton.enabled",
         "true"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "set spark.databricks.adviceGenerator.acceleratedWithPhoton.enabled = true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1057a64-d096-4fd4-91c9-557c71a16182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpLength</th><th>IntRate</th><th>cnt_loan_by_state</th><th>avg_loan_by_state</th><th>min_annual_income</th><th>max_annual_income</th><th>totalPayment_by_state</th></tr></thead><tbody><tr><td>1year</td><td>HighRate</td><td>50</td><td>15511.792100074785</td><td>16144.0</td><td>6000000.0</td><td>1.6845742348221095E9</td></tr><tr><td>Under1year</td><td>HighRate</td><td>50</td><td>15980.667546948356</td><td>16112.0</td><td>650000.0</td><td>1.687129366610441E9</td></tr><tr><td>3-5Years</td><td>MediumRate</td><td>50</td><td>14170.33729233472</td><td>16215.0</td><td>6200000.0</td><td>7.440206431133909E9</td></tr><tr><td>Under1year</td><td>MediumRate</td><td>50</td><td>14203.342335573741</td><td>16200.0</td><td>1300001.0</td><td>3.668164600199508E9</td></tr><tr><td>3-5Years</td><td>ExtremelyHighRate</td><td>50</td><td>17773.226341758895</td><td>16248.0</td><td>1100000.0</td><td>1.2390340980444412E9</td></tr><tr><td>1year</td><td>StandardRate</td><td>50</td><td>13561.58000986254</td><td>16380.0</td><td>1848400.0</td><td>3.0451219030125365E9</td></tr><tr><td>Under1year</td><td>ExtremelyHighRate</td><td>49</td><td>17769.116921558954</td><td>16500.0</td><td>585000.0</td><td>5.69117572940545E8</td></tr><tr><td>Under1year</td><td>lowRate</td><td>50</td><td>14589.907453726864</td><td>19000.0</td><td>780000.0</td><td>3.095344027199998E8</td></tr><tr><td>3-5Years</td><td>lowRate</td><td>50</td><td>14139.463474827246</td><td>20000.0</td><td>1574060.0</td><td>6.302428335599998E8</td></tr><tr><td>3-5Years</td><td>HighRate</td><td>50</td><td>16039.83564721813</td><td>16034.0</td><td>3000000.0</td><td>3.4527794636565747E9</td></tr><tr><td>1year</td><td>lowRate</td><td>48</td><td>14129.42999697611</td><td>17000.0</td><td>1050000.0</td><td>2.940917306600001E8</td></tr><tr><td>3-5Years</td><td>StandardRate</td><td>50</td><td>13928.975016966866</td><td>16123.0</td><td>6702150.0</td><td>6.394715844250635E9</td></tr><tr><td>1year</td><td>ExtremelyHighRate</td><td>50</td><td>16983.941445401793</td><td>16800.0</td><td>800000.0</td><td>5.967371959322517E8</td></tr><tr><td>1year</td><td>MediumRate</td><td>50</td><td>13787.683545797923</td><td>16128.0</td><td>5990095.0</td><td>3.6238409932779756E9</td></tr><tr><td>Under1year</td><td>StandardRate</td><td>50</td><td>14163.040993501656</td><td>16200.0</td><td>5499500.0</td><td>3.237480132350953E9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1year",
         "HighRate",
         50,
         15511.792100074785,
         16144.0,
         6000000.0,
         1.6845742348221095E9
        ],
        [
         "Under1year",
         "HighRate",
         50,
         15980.667546948356,
         16112.0,
         650000.0,
         1.687129366610441E9
        ],
        [
         "3-5Years",
         "MediumRate",
         50,
         14170.33729233472,
         16215.0,
         6200000.0,
         7.440206431133909E9
        ],
        [
         "Under1year",
         "MediumRate",
         50,
         14203.342335573741,
         16200.0,
         1300001.0,
         3.668164600199508E9
        ],
        [
         "3-5Years",
         "ExtremelyHighRate",
         50,
         17773.226341758895,
         16248.0,
         1100000.0,
         1.2390340980444412E9
        ],
        [
         "1year",
         "StandardRate",
         50,
         13561.58000986254,
         16380.0,
         1848400.0,
         3.0451219030125365E9
        ],
        [
         "Under1year",
         "ExtremelyHighRate",
         49,
         17769.116921558954,
         16500.0,
         585000.0,
         5.69117572940545E8
        ],
        [
         "Under1year",
         "lowRate",
         50,
         14589.907453726864,
         19000.0,
         780000.0,
         3.095344027199998E8
        ],
        [
         "3-5Years",
         "lowRate",
         50,
         14139.463474827246,
         20000.0,
         1574060.0,
         6.302428335599998E8
        ],
        [
         "3-5Years",
         "HighRate",
         50,
         16039.83564721813,
         16034.0,
         3000000.0,
         3.4527794636565747E9
        ],
        [
         "1year",
         "lowRate",
         48,
         14129.42999697611,
         17000.0,
         1050000.0,
         2.940917306600001E8
        ],
        [
         "3-5Years",
         "StandardRate",
         50,
         13928.975016966866,
         16123.0,
         6702150.0,
         6.394715844250635E9
        ],
        [
         "1year",
         "ExtremelyHighRate",
         50,
         16983.941445401793,
         16800.0,
         800000.0,
         5.967371959322517E8
        ],
        [
         "1year",
         "MediumRate",
         50,
         13787.683545797923,
         16128.0,
         5990095.0,
         3.6238409932779756E9
        ],
        [
         "Under1year",
         "StandardRate",
         50,
         14163.040993501656,
         16200.0,
         5499500.0,
         3.237480132350953E9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpLength",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "IntRate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cnt_loan_by_state",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_loan_by_state",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "min_annual_income",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "max_annual_income",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "totalPayment_by_state",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  T_len.EmpLength,\n",
    "  T_rate.IntRate,\n",
    "  count(DISTINCT T.addr_state) cnt_loan_by_state,\n",
    "  avg(loan_amnt) avg_loan_by_state,\n",
    "  min(DISTINCT annual_inc) as min_annual_income,\n",
    "  max(DISTINCT annual_inc) as max_annual_income,\n",
    "  sum(total_pymnt) totalPayment_by_state\n",
    "FROM\n",
    "  LendingClub_silver T\n",
    "  LEFT JOIN \n",
    "  (SELECT row_number() OVER(PARTITION BY addr_state ORDER BY avg_cur_bal DESC) as row_num_avgBal_state, *\n",
    "  FROM LendingClub_EmpLength) T_len on T_len.emp_length = T.emp_length and T_len.avg_cur_bal BETWEEN 1 AND 10\n",
    "  LEFT JOIN LendingClub_IntRate T_rate on T_rate.int_rate = T.int_rate\n",
    "WHERE\n",
    "  (annual_inc> 16000) AND loan_status == \"Current\"\n",
    "GROUP BY\n",
    "  1,\n",
    "  2\n",
    "HAVING EmpLength IN ('3-5Years', '1year', 'Under1year')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2666786534441247,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Photon Performance Monitoring in Spark UI and Logs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
